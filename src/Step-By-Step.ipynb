{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taoshen/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/taoshen/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/taoshen/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/taoshen/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/taoshen/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/taoshen/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/taoshen/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/taoshen/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/taoshen/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/taoshen/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/taoshen/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/taoshen/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FM\n",
    "$$\n",
    "\\hat y(x) = w_0+\\sum_{i=1}^n w_i x_i +\\sum_{i=1}^n \\sum_{j=i+1}^n ⟨vi,vj⟩ x_i x_j \\\\\n",
    "=w_0+\\sum_{i=1}^n w_i x_i + \\frac{1}{2} \\sum_{f=1}^{k} {\\left \\lgroup \\left(\\sum_{i=1}^{n} v_{i,f} x_i \\right)^2 - \\sum_{i=1}^{n} v_{i,f}^2 x_i^2\\right \\rgroup} \\qquad\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/ml-1m/ratings.dat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-40e781925af1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-40e781925af1>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mvec_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-40e781925af1>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mratings_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../data/ml-1m/ratings.dat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mratingsDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'::'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'item'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mratingsDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mratingsDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1145\u001b[0m                     \u001b[0;34m' \"python-fwf\")'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m                 )\n\u001b[0;32m-> 1147\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, **kwds)\u001b[0m\n\u001b[1;32m   2291\u001b[0m             \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2292\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2293\u001b[0;31m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2294\u001b[0m         )\n\u001b[1;32m   2295\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0;31m# No explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/ml-1m/ratings.dat'"
     ]
    }
   ],
   "source": [
    "from itertools import count\n",
    "import collections\n",
    "def vectorize_dic(dic,label2index=None,hold_num=None):\n",
    "    if not label2index:\n",
    "        d = count(0)\n",
    "        label2index = collections.defaultdict(lambda:next(d))\n",
    "    if not hold_num:\n",
    "        hold_num = len(label2index)\n",
    "    sample_num = len(list(dic.values())[0])\n",
    "    feature_num = len(dic.values())\n",
    "    total_num = sample_num * feature_num\n",
    "    \n",
    "    col_idx = np.empty(total_num,dtype=int)  # 列索引\n",
    "    \n",
    "    # i=0对所有的user做映射，i=1对所有的item做映射\n",
    "    i = 0\n",
    "    for feat,lis in dic.items():\n",
    "        col_idx[i::feature_num] = [label2index[str(feat)+str(el)] for el in lis]\n",
    "        i+=1\n",
    "    \n",
    "    row_idx = np.repeat(np.arange(sample_num),feature_num)\n",
    "    data = np.ones(total_num)\n",
    "    \n",
    "    left_data_index = np.where(col_idx<hold_num)\n",
    "    \n",
    "    return scipy.sparse.csr.csr_matrix((data[left_data_index],(row_idx[left_data_index],col_idx[left_data_index])),\n",
    "                                       shape=(sample_num,feature_num)),label2index\n",
    "    \n",
    "    \n",
    "\n",
    "def load_data():\n",
    "    ratings_path = '../data/ml-1m/ratings.dat'\n",
    "    ratingsDF = pd.read_csv(ratings_path,index_col=None,sep='::',header=None,names=['user','item','rating','timestamp'],nrows=1000)\n",
    "    \n",
    "    ratingsDF = ratingsDF.sample(frac=1.0)\n",
    "    cut_idx = int(ratingsDF.shape[0]*0.7)\n",
    "    train,test = ratingsDF.iloc[:cut_idx],ratingsDF.iloc[cut_idx:]\n",
    "    \n",
    "    x_train,label2index = vectorize_dic({'users':train['user'].values,'item':train['item'].values})\n",
    "    x_test,_ = vectorize_dic({'users':test['user'].values,'item':test['item'].values},label2index,x_train.shape[1])\n",
    "    \n",
    "    x_train,x_test = x_train.todense(),x_test.todense()\n",
    "    y_train,y_test = train['rating'].values,test['rating'].values\n",
    "    \n",
    "    return x_train,x_test,y_train,y_test\n",
    "    \n",
    "def batcher(x,y,batch_size=-1):\n",
    "    sample_num = x.shape[0]\n",
    "    if batch_size == -1:\n",
    "        batch_size = sample_num\n",
    "    for i in range(0,sample_num,batch_size):\n",
    "        upper_bound = min(i+batch_size,sample_num)\n",
    "        batch_x = x[i:upper_bound]\n",
    "        batch_y = y[i:upper_bound]\n",
    "        yield (batch_x,batch_y)\n",
    "\n",
    "        \n",
    "# 所有feat（onehot之后）映射到一个vec_embedding\n",
    "    \n",
    "def main():\n",
    "    x_train,x_test,y_train,y_test = load_data()\n",
    "    \n",
    "    vec_dim = 10\n",
    "    \n",
    "    sample_num,feat_num = x_train.shape\n",
    "    batch_size = 32\n",
    "    epochs = 20\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    x = tf.placeholder(tf.float32,shape=[None,feat_num],name='x')\n",
    "    y = tf.placeholder(tf.float32,shape=[None,1],name='y')\n",
    "    \n",
    "    w0 = tf.get_variable(name='bias',shape=(1),dtype=tf.float32)\n",
    "    W = tf.get_variable(name='linear_w',shape=(feat_num,1),dtype=tf.float32)\n",
    "    #这里也可以初始化为shape=(feat_num),后面使用multiply，效果一致\n",
    "    V = tf.get_variable(name='interaction_w',shape=(feat_num,vec_dim),dtype=tf.float32)\n",
    "    \n",
    "    linear_part = w0 + tf.reduce_sum(tf.matmul(x,W),axis=1,keep_dims=True)\n",
    "    interaction_part = 0.5 * tf.reduce_sum(tf.square(tf.matmul(x,V))-tf.matmul(tf.square(x),tf.square(V)),axis=1,keep_dims=True)\n",
    "    y_hat = linear_part + interaction_part\n",
    "    loss = tf.reduce_mean(tf.square(y-y_hat))\n",
    "    \n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for e in range(epochs):\n",
    "            print('Epoch:%i'%e)\n",
    "            step = 0\n",
    "            for batch_x,batch_y in batcher(x_train,y_train,batch_size=batch_size):\n",
    "                sess.run(train_op,feed_dict={x:batch_x,y:batch_y.reshape(-1,1)})\n",
    "                step += 1\n",
    "                if step % 10 == 0:\n",
    "                    test_loss = sess.run(loss,feed_dict={x:x_test,y:y_test.reshape(-1,1)})\n",
    "                    print('Step:%d,test MSE:%.3f'%(step,test_loss))\n",
    "                    \n",
    "tf.reset_default_graph()     \n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_path = '../data/ctr/train.csv'\n",
    "test_path = '../data/ctr/test.csv'\n",
    "\n",
    "dfTrain = pd.read_csv(train_path)\n",
    "dfTest = pd.read_csv(test_path)\n",
    "\n",
    "def preprocess(df):\n",
    "    cols = [c for c in df.columns if c not in ['id','target']]\n",
    "    df[\"missing_feat\"] = np.sum((df[cols] == -1).values, axis=1)\n",
    "    df['ps_car_13_x_ps_reg_03'] = df['ps_car_13'] * df['ps_reg_03']\n",
    "    return df\n",
    "    \n",
    "\n",
    "dfTrain,dfTest = preprocess(dfTrain),preprocess(dfTest)\n",
    "IGNORE_COLS = [\n",
    "    \"id\", \"target\",\n",
    "    \"ps_calc_01\", \"ps_calc_02\", \"ps_calc_03\", \"ps_calc_04\",\n",
    "    \"ps_calc_05\", \"ps_calc_06\", \"ps_calc_07\", \"ps_calc_08\",\n",
    "    \"ps_calc_09\", \"ps_calc_10\", \"ps_calc_11\", \"ps_calc_12\",\n",
    "    \"ps_calc_13\", \"ps_calc_14\",\n",
    "    \"ps_calc_15_bin\", \"ps_calc_16_bin\", \"ps_calc_17_bin\",\n",
    "    \"ps_calc_18_bin\", \"ps_calc_19_bin\", \"ps_calc_20_bin\"\n",
    "]\n",
    "NUMERIC_COLS = [\n",
    "    \"ps_reg_01\", \"ps_reg_02\", \"ps_reg_03\",\n",
    "    \"ps_car_12\", \"ps_car_13\", \"ps_car_14\", \"ps_car_15\"\n",
    "]\n",
    "cols = [c for c in dfTrain.columns if (not c in IGNORE_COLS)]\n",
    "\n",
    "x_train,x_test = dfTrain[cols].values,dfTest[cols].values\n",
    "y_train = dfTrain['target'].values\n",
    "ids_test = dfTest['id'].values\n",
    "\n",
    "\n",
    "cat_features_indices = [i for i,c in enumerate(cols)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DeepFM数据预处理\n",
    "dfAll = pd.concat([dfTrain,dfTest])\n",
    "\n",
    "feature_dic = {}\n",
    "total_feature = 0\n",
    "for col in dfAll:\n",
    "    if col in IGNORE_COLS:\n",
    "        continue\n",
    "    elif col in NUMERIC_COLS:\n",
    "        feature_dic[col] = total_feature\n",
    "        total_feature += 1\n",
    "    else:\n",
    "        unique_val = dfAll[col].unique()\n",
    "        feature_dic[col] = dict(zip(unique_val,range(total_feature,total_feature+len(unique_val))))\n",
    "        total_feature += len(unique_val)\n",
    "        \n",
    "def process(df,feature_dic):\n",
    "    dfi = df.copy()\n",
    "    dfv = df.copy()\n",
    "    for col in df.columns:\n",
    "        if col in IGNORE_COLS:\n",
    "            dfi.drop(col,axis=1,inplace=True)\n",
    "            dfv.drop(col,axis=1,inplace=True)\n",
    "        elif col in NUMERIC_COLS:\n",
    "            dfi[col] = feature_dic[col]\n",
    "        else:\n",
    "            dfi[col] = dfi[col].map(feature_dic[col])\n",
    "            dfv[col] = 1\n",
    "    return dfi,dfv\n",
    "        \n",
    "        \n",
    "train_feature_index,train_feature_value = process(dfTrain,feature_dic)   \n",
    "test_feature_index,test_feature_value = process(dfTest,feature_dic)   \n",
    "\n",
    "\n",
    "dfm_params = {\n",
    "    'use_fm':True,\n",
    "    'use_deep':True,\n",
    "    'embedding_size':8,\n",
    "    'dropout_fm':[1,1],\n",
    "    'deep_layers':[32,32],\n",
    "    'dropout_deep':[0.5,0.5,0.5],\n",
    "    'deep_layer_activation':tf.nn.relu,\n",
    "    'epoch':30,\n",
    "    'learning_rate':0.01,\n",
    "    'batch_size':64,\n",
    "    'optimizer':'adam',\n",
    "    'batch_norm':1,\n",
    "    'batch_decay':0.95,\n",
    "    'l2_reg':0.01,\n",
    "    'verbose':True,\n",
    "    'eval_metric':'gini_norm',\n",
    "    'random_seed':3\n",
    "}\n",
    "\n",
    "dfm_params['feature_size'] = total_feature\n",
    "dfm_params['field_size'] = train_feature_index.shape[1]\n",
    "\n",
    "# 构建图\n",
    "tf.reset_default_graph()  \n",
    "\n",
    "\n",
    "feat_index = tf.placeholder(tf.int32,shape=[None,None],name='index')\n",
    "feat_value = tf.placeholder(tf.float32,shape=[None,None],name='value')\n",
    "label = tf.placeholder(tf.float32,shape=[None,1],name='label')\n",
    "\n",
    "weights = {}\n",
    "weights['feature_embeddings'] = tf.Variable(tf.random_normal([dfm_params['feature_size'],dfm_params['embedding_size']],0.,0.01),name='feature_embeddings')\n",
    "weights['feature_bias'] = tf.Variable(tf.random_normal([dfm_params['feature_size'],1],0.,0.1),name='feature_bias')\n",
    "\n",
    "\n",
    "num_layers = len(dfm_params['deep_layers'])\n",
    "input_size = dfm_params['field_size']*dfm_params['embedding_size']  # 每一个field做embedding\n",
    "glorot = np.sqrt(2./(input_size+dfm_params['deep_layers'][0]))\n",
    "weights['layer_0'] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=(input_size,dfm_params['deep_layers'][0])),dtype=np.float32)\n",
    "'''weights中bias取一个维度即可，且输入层不需要bias'''\n",
    "weights['bias_0'] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=(dfm_params['deep_layers'][0])),dtype=np.float32)\n",
    "\n",
    "for i in range(1,num_layers):\n",
    "    glorot = np.sqrt(2./(dfm_params['deep_layers'][i-1]+dfm_params['deep_layers'][i]))\n",
    "    weights['layer_'+str(i)] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=(dfm_params['deep_layers'][i-1],dfm_params['deep_layers'][i])),dtype=np.float32)\n",
    "    weights['bias_'+str(i)] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=(dfm_params['deep_layers'][i])),dtype=np.float32)\n",
    "\n",
    "'''最后一层为FM层+DNN最后一层'''\n",
    "input_size = dfm_params['deep_layers'][-1]+dfm_params['field_size']+dfm_params['embedding_size']\n",
    "glorot = np.sqrt(2./(input_size+1))\n",
    "\n",
    "weights['concat_project'] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=(input_size,1)),dtype=np.float32)\n",
    "weights['concat_bias'] = tf.Variable(tf.constant(0.01),dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''embedding'''\n",
    "embeddings = tf.nn.embedding_lookup(weights['feature_embeddings'],feat_index) \n",
    "reshaped_feat_value = tf.reshape(feat_value,[-1,dfm_params['field_size'],1])\n",
    "embeddings = tf.multiply(embeddings,reshaped_feat_value)\n",
    "\n",
    "'''fm part'''\n",
    "'''一阶'''\n",
    "fm_first_order = tf.nn.embedding_lookup(weights['feature_embeddings'],feat_index)\n",
    "fm_first_order = tf.reduce_sum(tf.multiply(fm_first_order,reshaped_feat_value),2)\n",
    "\n",
    "summed_feature_emb = tf.reduce_sum(embeddings,1)\n",
    "summed_feature_emb_square = tf.square(summed_feature_emb)\n",
    "\n",
    "squared_feature_emb = tf.square(embeddings)\n",
    "squared_sum_feature_emb = tf.reduce_sum(squared_feature_emb,1)\n",
    "\n",
    "fm_second_order = 0.5 * tf.subtract(summed_feature_emb_square,squared_sum_feature_emb)\n",
    "\n",
    "'''deep part'''\n",
    "y_deep = tf.reshape(embeddings,shape=[-1,dfm_params['field_size']*dfm_params['embedding_size']])\n",
    "\n",
    "for i in range(num_layers):\n",
    "    y_deep = tf.add(tf.matmul(y_deep,weights['layer_'+str(i)]),weights['bias_'+str(i)])\n",
    "    y_deep = tf.nn.relu(y_deep)\n",
    "\n",
    "concat_input = tf.concat([fm_first_order,fm_second_order,y_deep],axis=1)\n",
    "out = tf.nn.sigmoid(tf.add(tf.matmul(concat_input,weights['concat_project']),weights['concat_bias']))\n",
    "\n",
    "\n",
    "loss = tf.losses.log_loss(label,out)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate = dfm_params['learning_rate'],beta1=0.9,beta2=0.999,epsilon=1e-8).minimize(loss)\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(dfm_params['epoch']):\n",
    "        '''loss与train_op顺序不可换'''\n",
    "        epoch_loss,_ = sess.run([loss,train_op],feed_dict={feat_index:train_feature_index,feat_value:train_feature_value,label:y_train.reshape(-1,1)})\n",
    "        print('epoch:%s,Loss:%s'%(i,epoch_loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCN\n",
    "$$x_{l+1} = x_0 x_l^T w_l + b_l + x_l = f(x_l,w_l,b_l)+x_l$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCN处理\n",
    "dfAll = pd.concat([dfTrain,dfTest])\n",
    "\n",
    "feature_dic = {}\n",
    "total_feature = 0\n",
    "for col in dfAll:\n",
    "    if col in IGNORE_COLS or col in NUMERIC_COLS:\n",
    "        continue\n",
    "    else:\n",
    "        unique_val = dfAll[col].unique()\n",
    "        feature_dic[col] = dict(zip(unique_val,range(total_feature,total_feature+len(unique_val))))\n",
    "        total_feature += len(unique_val)\n",
    "        \n",
    "'''\n",
    "主要将离散特征和连续特征分开，连续特征不在转换成embedding进行输入\n",
    "'''     \n",
    "def process(df,feature_dic):\n",
    "    num_val = df[NUMERIC_COLS].values  #.tolist()\n",
    "    '''只保留类别特征'''\n",
    "    dfi = df.copy()\n",
    "    dfi.drop(NUMERIC_COLS,axis=1,inplace=True)\n",
    "    dfv = dfi.copy()\n",
    "    for col in dfi.columns:\n",
    "        if col in IGNORE_COLS:\n",
    "            dfi.drop(col,axis=1,inplace=True)\n",
    "            dfv.drop(col,axis=1,inplace=True)\n",
    "        else:\n",
    "            dfi[col] = dfi[col].map(feature_dic[col])\n",
    "            dfv[col] = 1\n",
    "    dfi = dfi.values\n",
    "    dfv = dfv.values\n",
    "    \n",
    "    return dfi,dfv,num_val\n",
    "    \n",
    "train_cate_feature_index,train_cate_feature_value,train_num_feature_val = process(dfTrain,feature_dic)   \n",
    "test_cate_feature_index,test_feature_cate_value,test_num_feature_val = process(dfTest,feature_dic)   \n",
    "      \n",
    "\n",
    "dcn_params = {\n",
    "    \"embedding_size\": 8,\n",
    "    \"deep_layers\": [32, 32],\n",
    "    \"dropout_deep\": [0.5, 0.5, 0.5],\n",
    "    \"deep_layers_activation\": tf.nn.relu,\n",
    "    'loss' : 'logloss',\n",
    "    \"epoch\": 30,\n",
    "    \"batch_size\": 1024,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"optimizer_type\": \"adam\",\n",
    "    \"batch_norm\": 1,\n",
    "    \"batch_norm_decay\": 0.995,\n",
    "    \"l2_reg\": 0.01,\n",
    "    \"verbose\": True,\n",
    "    \"random_seed\": 2019,\n",
    "    \"cross_layer_num\":3\n",
    "}\n",
    "\n",
    "dcn_params['cate_feature_size'] = total_feature\n",
    "dcn_params['field_size'] = train_cate_feature_index.shape[1]\n",
    "dcn_params['numeric_feature_size'] = train_num_feature_val.shape[1]\n",
    "total_size = dcn_params['field_size']*dcn_params['embedding_size']+dcn_params['numeric_feature_size']  # 每一个field做embedding\n",
    "\n",
    "# 构建图\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(dcn_params['random_seed'])\n",
    "\n",
    "feat_index = tf.placeholder(tf.int32,shape=[None,None],name='feat_index')\n",
    "feat_value = tf.placeholder(tf.float32,shape=[None,None],name='feat_value')\n",
    "numeric_value = tf.placeholder(tf.float32,shape=[None,None],name='numeric_val')\n",
    "label = tf.placeholder(tf.float32,shape=[None,1],name='label')\n",
    "\n",
    "\n",
    "weights = {}\n",
    "weights['feature_embedding'] = tf.Variable(tf.random_normal(shape=[dcn_params['cate_feature_size'],dcn_params['embedding_size']],mean=0.,stddev=0.01),name='feature_embeddings')\n",
    "weights['feature_bias'] = tf.Variable(tf.random_normal(shape=[dcn_params['cate_feature_size']],mean=0,stddev=1),name='feature_bias')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''deep'''\n",
    "input_size = total_size\n",
    "glorot = np.sqrt(2./(input_size+dcn_params['deep_layers'][0]))\n",
    "weights['deep_layers_0'] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=[input_size,dcn_params['deep_layers'][0]]),dtype=np.float32,name='layers_0')\n",
    "weights['deep_bias_0'] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=[dcn_params['deep_layers'][0]]),dtype=np.float32,name='bias_0')\n",
    "\n",
    "\n",
    "num_layers = len(dcn_params['deep_layers'])\n",
    "for i in range(1,num_layers):\n",
    "    glorot = np.sqrt(2./(dcn_params['deep_layers'][i-1]+dcn_params['deep_layers'][i]))\n",
    "    weights['deep_layers_'+str(i)] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=[dcn_params['deep_layers'][i-1],dcn_params['deep_layers'][i]]),dtype=np.float32,name='deep_layers_'+str(i))\n",
    "    weights['deep_bias_'+str(i)] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=[dcn_params['deep_layers'][i]]),dtype=np.float32,name='deep_bias_'+str(i))\n",
    "\n",
    "\n",
    "# glorot = np.sqrt(2./(input_size*2))    # 初始化\n",
    "for i in range(dcn_params['cross_layer_num']):\n",
    "    weights['cross_layers_'+str(i)] = tf.Variable(np.random.normal(loc=0,scale=0.01,size=[total_size,1]),dtype=np.float32,name='cross_layers_'+str(i))\n",
    "    weights['cross_bias_'+str(i)] = tf.Variable(np.random.normal(loc=0,scale=0.01,size=[total_size,1]),dtype=np.float32,name='cross_bias_'+str(i))\n",
    "\n",
    "input_size += dcn_params['deep_layers'][-1]     \n",
    "glorot = np.sqrt(2./(input_size+1))\n",
    "weights['concat_projection'] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=[input_size,1]),dtype=np.float32,name='concat_projection')\n",
    "weights['concat_bias'] = tf.Variable(tf.constant(0.01),dtype=np.float32)                                                        \n",
    "                                                                                             \n",
    "                                                                                             \n",
    "embeddings = tf.nn.embedding_lookup(weights['feature_embedding'],feat_index)\n",
    "reshaped_feat_value = tf.reshape(feat_value,[-1,dcn_params['field_size'],1])\n",
    "embeddings = tf.multiply(embeddings,reshaped_feat_value)\n",
    "\n",
    "x0 = tf.concat([numeric_value,tf.reshape(embeddings,[-1,dcn_params['field_size']*dcn_params['embedding_size']])],axis=1)\n",
    "\n",
    "'''deep part'''\n",
    "y_deep = tf.nn.dropout(x0,dcn_params['dropout_deep'][0])\n",
    "for i in range(num_layers):\n",
    "    y_deep = tf.add(tf.matmul(y_deep,weights['deep_layers_'+str(i)]),weights['deep_bias_'+str(i)])\n",
    "    y_deep = tf.nn.relu(y_deep)\n",
    "    y_deep = tf.nn.dropout(y_deep,dcn_params['dropout_deep'][i+1])\n",
    "    \n",
    "'''cross part'''\n",
    "_x0 = tf.reshape(x0,[-1,total_size,1])\n",
    "x_l = _x0\n",
    "for l in range(dcn_params['cross_layer_num']):\n",
    "#     x_l = tf.tensordot(tf.matmul(_x0, x_l, transpose_b=True),\n",
    "#                                     weights[\"cross_layers_%d\" % l],1) + weights[\"cross_bias_%d\" % l] + x_l\n",
    "    # 注意计算顺序，可以加速很多\n",
    "    x_l = tf.tensordot(tf.reshape(x_l,[-1,1,total_size]),weights[\"cross_layers_%d\" % l],1) * _x0 + weights[\"cross_bias_%d\" % l] + x_l\n",
    "\n",
    "\n",
    "cross_network_out = tf.reshape(x_l,[-1,total_size])\n",
    "\n",
    "\n",
    "\n",
    "concat_input = tf.concat([y_deep,cross_network_out],axis=1)\n",
    "out = tf.add(tf.matmul(concat_input,weights['concat_projection']),weights['concat_bias'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if dcn_params['loss'] == 'logloss':\n",
    "    out = tf.nn.sigmoid(out)\n",
    "    loss = tf.losses.log_loss(label,out)\n",
    "elif dcn_params['loss'] == 'mse':\n",
    "    loss = tf.nn.l2_loss(tf.subtract(label,out))\n",
    "\n",
    "if dcn_params['l2_reg']>0:\n",
    "    loss += tf.contrib.layers.l2_regularizer(dcn_params['l2_reg'])(weights['concat_projection'])\n",
    "    for i in range(num_layers):\n",
    "        loss += tf.contrib.layers.l2_regularizer(dcn_params['l2_reg'])(weights['deep_layers_%d'%i])\n",
    "    for i in range(dcn_params['cross_layer_num']):\n",
    "        loss += tf.contrib.layers.l2_regularizer(dcn_params['l2_reg'])(weights['cross_layers_%d'%i])\n",
    "if dcn_params['optimizer_type'] == 'adam':\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate = dcn_params['learning_rate'],beta1=0.9,beta2=0.99,epsilon=1e-8).minimize(loss)\n",
    "elif dcn_params['optimizer_type'] == 'adagrad':\n",
    "    train_op = tf.train.AdagradOptimizer(learning_rate = dcn_params['learning_rate'],initial_accumulator_value=1e-8).minimize(loss)\n",
    "elif dcn_params['optimizer_type'] == 'gd':\n",
    "    train_op = tf.train.GradientDescentOptimizer(learning_rate=dcn_params['learning_rate']).minimize(loss)\n",
    "elif dcn_params['optimizer_type'] == 'momentum':\n",
    "    train_op = tf.train.MomentumOptimizer(learning_rate = dcn_params['learning_rate'],momentum=0.95).minimize(loss)\n",
    "        \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "def shuffle_in_union_scary(a,b,c,d):\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(b)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(c)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(d)\n",
    "\n",
    "def getbatch(Xi,Xv,Xv2,y,batch_size,index):\n",
    "    start = batch_size * index\n",
    "    end = min(batch_size * (index + 1),len(y))\n",
    "    return Xi[start:end],Xv[start:end],Xv2[start:end],y[start:end]\n",
    "  \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for e in range(dcn_params['epoch']):\n",
    "        shuffle_in_union_scary(train_cate_feature_index,train_cate_feature_value,train_num_feature_val,y_train)\n",
    "        for i in range(len(y_train)//dcn_params['batch_size']):\n",
    "            cate_Xi_batch,cate_Xv_batch,numeric_Xv_batch,y_batch = getbatch(train_cate_feature_index,train_cate_feature_value,train_num_feature_val,y_train,dcn_params['batch_size'],e)\n",
    "            '''注意取名，不能与loss重复'''\n",
    "            cur_loss,_ = sess.run([loss,train_op],feed_dict={feat_index:cate_Xi_batch,feat_value:cate_Xv_batch,numeric_value:numeric_Xv_batch,label:y_batch.reshape(-1,1)})\n",
    "        print('Epoch%i : Loss %.2f'%(e,cur_loss))\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
