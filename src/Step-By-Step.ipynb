{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taoshen/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/taoshen/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/taoshen/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/taoshen/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/taoshen/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/taoshen/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/taoshen/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/taoshen/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/taoshen/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/taoshen/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/taoshen/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/taoshen/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FM\n",
    "$$\n",
    "\\hat y(x) = w_0+\\sum_{i=1}^n w_i x_i +\\sum_{i=1}^n \\sum_{j=i+1}^n ⟨vi,vj⟩ x_i x_j \\\\\n",
    "=w_0+\\sum_{i=1}^n w_i x_i + \\frac{1}{2} \\sum_{f=1}^{k} {\\left \\lgroup \\left(\\sum_{i=1}^{n} v_{i,f} x_i \\right)^2 - \\sum_{i=1}^{n} v_{i,f}^2 x_i^2\\right \\rgroup} \\qquad\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/taoshen/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From <ipython-input-2-40e781925af1>:78: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Epoch:0\n",
      "Step:10,test MSE:6.162\n",
      "Step:20,test MSE:5.734\n",
      "Epoch:1\n",
      "Step:10,test MSE:5.256\n",
      "Step:20,test MSE:4.883\n",
      "Epoch:2\n",
      "Step:10,test MSE:4.473\n",
      "Step:20,test MSE:4.156\n",
      "Epoch:3\n",
      "Step:10,test MSE:3.809\n",
      "Step:20,test MSE:3.542\n",
      "Epoch:4\n",
      "Step:10,test MSE:3.253\n",
      "Step:20,test MSE:3.032\n",
      "Epoch:5\n",
      "Step:10,test MSE:2.794\n",
      "Step:20,test MSE:2.614\n",
      "Epoch:6\n",
      "Step:10,test MSE:2.421\n",
      "Step:20,test MSE:2.276\n",
      "Epoch:7\n",
      "Step:10,test MSE:2.122\n",
      "Step:20,test MSE:2.007\n",
      "Epoch:8\n",
      "Step:10,test MSE:1.887\n",
      "Step:20,test MSE:1.798\n",
      "Epoch:9\n",
      "Step:10,test MSE:1.707\n",
      "Step:20,test MSE:1.639\n",
      "Epoch:10\n",
      "Step:10,test MSE:1.571\n",
      "Step:20,test MSE:1.521\n",
      "Epoch:11\n",
      "Step:10,test MSE:1.471\n",
      "Step:20,test MSE:1.436\n",
      "Epoch:12\n",
      "Step:10,test MSE:1.402\n",
      "Step:20,test MSE:1.377\n",
      "Epoch:13\n",
      "Step:10,test MSE:1.354\n",
      "Step:20,test MSE:1.339\n",
      "Epoch:14\n",
      "Step:10,test MSE:1.325\n",
      "Step:20,test MSE:1.316\n",
      "Epoch:15\n",
      "Step:10,test MSE:1.308\n",
      "Step:20,test MSE:1.304\n",
      "Epoch:16\n",
      "Step:10,test MSE:1.301\n",
      "Step:20,test MSE:1.300\n",
      "Epoch:17\n",
      "Step:10,test MSE:1.300\n",
      "Step:20,test MSE:1.301\n",
      "Epoch:18\n",
      "Step:10,test MSE:1.303\n",
      "Step:20,test MSE:1.305\n",
      "Epoch:19\n",
      "Step:10,test MSE:1.308\n",
      "Step:20,test MSE:1.311\n"
     ]
    }
   ],
   "source": [
    "from itertools import count\n",
    "import collections\n",
    "def vectorize_dic(dic,label2index=None,hold_num=None):\n",
    "    if not label2index:\n",
    "        d = count(0)\n",
    "        label2index = collections.defaultdict(lambda:next(d))\n",
    "    if not hold_num:\n",
    "        hold_num = len(label2index)\n",
    "    sample_num = len(list(dic.values())[0])\n",
    "    feature_num = len(dic.values())\n",
    "    total_num = sample_num * feature_num\n",
    "    \n",
    "    col_idx = np.empty(total_num,dtype=int)  # 列索引\n",
    "    \n",
    "    # i=0对所有的user做映射，i=1对所有的item做映射\n",
    "    i = 0\n",
    "    for feat,lis in dic.items():\n",
    "        col_idx[i::feature_num] = [label2index[str(feat)+str(el)] for el in lis]\n",
    "        i+=1\n",
    "    \n",
    "    row_idx = np.repeat(np.arange(sample_num),feature_num)\n",
    "    data = np.ones(total_num)\n",
    "    \n",
    "    left_data_index = np.where(col_idx<hold_num)\n",
    "    \n",
    "    return scipy.sparse.csr.csr_matrix((data[left_data_index],(row_idx[left_data_index],col_idx[left_data_index])),\n",
    "                                       shape=(sample_num,feature_num)),label2index\n",
    "    \n",
    "    \n",
    "\n",
    "def load_data():\n",
    "    ratings_path = '../data/ml-1m/ratings.dat'\n",
    "    ratingsDF = pd.read_csv(ratings_path,index_col=None,sep='::',header=None,names=['user','item','rating','timestamp'],nrows=1000)\n",
    "    \n",
    "    ratingsDF = ratingsDF.sample(frac=1.0)\n",
    "    cut_idx = int(ratingsDF.shape[0]*0.7)\n",
    "    train,test = ratingsDF.iloc[:cut_idx],ratingsDF.iloc[cut_idx:]\n",
    "    \n",
    "    x_train,label2index = vectorize_dic({'users':train['user'].values,'item':train['item'].values})\n",
    "    x_test,_ = vectorize_dic({'users':test['user'].values,'item':test['item'].values},label2index,x_train.shape[1])\n",
    "    \n",
    "    x_train,x_test = x_train.todense(),x_test.todense()\n",
    "    y_train,y_test = train['rating'].values,test['rating'].values\n",
    "    \n",
    "    return x_train,x_test,y_train,y_test\n",
    "    \n",
    "def batcher(x,y,batch_size=-1):\n",
    "    sample_num = x.shape[0]\n",
    "    if batch_size == -1:\n",
    "        batch_size = sample_num\n",
    "    for i in range(0,sample_num,batch_size):\n",
    "        upper_bound = min(i+batch_size,sample_num)\n",
    "        batch_x = x[i:upper_bound]\n",
    "        batch_y = y[i:upper_bound]\n",
    "        yield (batch_x,batch_y)\n",
    "\n",
    "        \n",
    "# 所有feat（onehot之后）映射到一个vec_embedding\n",
    "    \n",
    "def main():\n",
    "    x_train,x_test,y_train,y_test = load_data()\n",
    "    \n",
    "    vec_dim = 10\n",
    "    \n",
    "    sample_num,feat_num = x_train.shape\n",
    "    batch_size = 32\n",
    "    epochs = 20\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    x = tf.placeholder(tf.float32,shape=[None,feat_num],name='x')\n",
    "    y = tf.placeholder(tf.float32,shape=[None,1],name='y')\n",
    "    \n",
    "    w0 = tf.get_variable(name='bias',shape=(1),dtype=tf.float32)\n",
    "    W = tf.get_variable(name='linear_w',shape=(feat_num,1),dtype=tf.float32)\n",
    "    #这里也可以初始化为shape=(feat_num),后面使用multiply，效果一致\n",
    "    V = tf.get_variable(name='interaction_w',shape=(feat_num,vec_dim),dtype=tf.float32)\n",
    "    \n",
    "    linear_part = w0 + tf.reduce_sum(tf.matmul(x,W),axis=1,keep_dims=True)\n",
    "    interaction_part = 0.5 * tf.reduce_sum(tf.square(tf.matmul(x,V))-tf.matmul(tf.square(x),tf.square(V)),axis=1,keep_dims=True)\n",
    "    y_hat = linear_part + interaction_part\n",
    "    loss = tf.reduce_mean(tf.square(y-y_hat))\n",
    "    \n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for e in range(epochs):\n",
    "            print('Epoch:%i'%e)\n",
    "            step = 0\n",
    "            for batch_x,batch_y in batcher(x_train,y_train,batch_size=batch_size):\n",
    "                sess.run(train_op,feed_dict={x:batch_x,y:batch_y.reshape(-1,1)})\n",
    "                step += 1\n",
    "                if step % 10 == 0:\n",
    "                    test_loss = sess.run(loss,feed_dict={x:x_test,y:y_test.reshape(-1,1)})\n",
    "                    print('Step:%d,test MSE:%.3f'%(step,test_loss))\n",
    "                    \n",
    "tf.reset_default_graph()     \n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_path = '../data/ctr/train.csv'\n",
    "test_path = '../data/ctr/test.csv'\n",
    "\n",
    "dfTrain = pd.read_csv(train_path)\n",
    "dfTest = pd.read_csv(test_path)\n",
    "\n",
    "def preprocess(df):\n",
    "    cols = [c for c in df.columns if c not in ['id','target']]\n",
    "    df[\"missing_feat\"] = np.sum((df[cols] == -1).values, axis=1)\n",
    "    df['ps_car_13_x_ps_reg_03'] = df['ps_car_13'] * df['ps_reg_03']\n",
    "    return df\n",
    "    \n",
    "\n",
    "dfTrain,dfTest = preprocess(dfTrain),preprocess(dfTest)\n",
    "IGNORE_COLS = [\n",
    "    \"id\", \"target\",\n",
    "    \"ps_calc_01\", \"ps_calc_02\", \"ps_calc_03\", \"ps_calc_04\",\n",
    "    \"ps_calc_05\", \"ps_calc_06\", \"ps_calc_07\", \"ps_calc_08\",\n",
    "    \"ps_calc_09\", \"ps_calc_10\", \"ps_calc_11\", \"ps_calc_12\",\n",
    "    \"ps_calc_13\", \"ps_calc_14\",\n",
    "    \"ps_calc_15_bin\", \"ps_calc_16_bin\", \"ps_calc_17_bin\",\n",
    "    \"ps_calc_18_bin\", \"ps_calc_19_bin\", \"ps_calc_20_bin\"\n",
    "]\n",
    "NUMERIC_COLS = [\n",
    "    \"ps_reg_01\", \"ps_reg_02\", \"ps_reg_03\",\n",
    "    \"ps_car_12\", \"ps_car_13\", \"ps_car_14\", \"ps_car_15\"\n",
    "]\n",
    "cols = [c for c in dfTrain.columns if (not c in IGNORE_COLS)]\n",
    "\n",
    "x_train,x_test = dfTrain[cols].values,dfTest[cols].values\n",
    "y_train = dfTrain['target'].values\n",
    "ids_test = dfTest['id'].values\n",
    "\n",
    "\n",
    "cat_features_indices = [i for i,c in enumerate(cols)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/taoshen/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "epoch:0,Loss:0.7773253\n",
      "epoch:1,Loss:0.5417052\n",
      "epoch:2,Loss:0.35673484\n",
      "epoch:3,Loss:0.22753826\n",
      "epoch:4,Loss:0.16803075\n",
      "epoch:5,Loss:0.15897305\n",
      "epoch:6,Loss:0.17649129\n",
      "epoch:7,Loss:0.19727606\n",
      "epoch:8,Loss:0.20989627\n",
      "epoch:9,Loss:0.21218628\n",
      "epoch:10,Loss:0.2060652\n",
      "epoch:11,Loss:0.19401805\n",
      "epoch:12,Loss:0.17890438\n",
      "epoch:13,Loss:0.16328596\n",
      "epoch:14,Loss:0.14911337\n",
      "epoch:15,Loss:0.13857953\n",
      "epoch:16,Loss:0.13112995\n",
      "epoch:17,Loss:0.1255495\n",
      "epoch:18,Loss:0.12259299\n",
      "epoch:19,Loss:0.11945083\n",
      "epoch:20,Loss:0.11505127\n",
      "epoch:21,Loss:0.10915083\n",
      "epoch:22,Loss:0.100132756\n",
      "epoch:23,Loss:0.08852969\n",
      "epoch:24,Loss:0.076438524\n",
      "epoch:25,Loss:0.06531809\n",
      "epoch:26,Loss:0.056068007\n",
      "epoch:27,Loss:0.046559855\n",
      "epoch:28,Loss:0.036692847\n",
      "epoch:29,Loss:0.027390907\n"
     ]
    }
   ],
   "source": [
    "#DeepFM数据预处理\n",
    "dfAll = pd.concat([dfTrain,dfTest])\n",
    "\n",
    "feature_dic = {}\n",
    "total_feature = 0\n",
    "for col in dfAll:\n",
    "    if col in IGNORE_COLS:\n",
    "        continue\n",
    "    elif col in NUMERIC_COLS:\n",
    "        feature_dic[col] = total_feature\n",
    "        total_feature += 1\n",
    "    else:\n",
    "        unique_val = dfAll[col].unique()\n",
    "        feature_dic[col] = dict(zip(unique_val,range(total_feature,total_feature+len(unique_val))))\n",
    "        total_feature += len(unique_val)\n",
    "        \n",
    "def process(df,feature_dic):\n",
    "    dfi = df.copy()\n",
    "    dfv = df.copy()\n",
    "    for col in df.columns:\n",
    "        if col in IGNORE_COLS:\n",
    "            dfi.drop(col,axis=1,inplace=True)\n",
    "            dfv.drop(col,axis=1,inplace=True)\n",
    "        elif col in NUMERIC_COLS:\n",
    "            dfi[col] = feature_dic[col]\n",
    "        else:\n",
    "            dfi[col] = dfi[col].map(feature_dic[col])\n",
    "            dfv[col] = 1\n",
    "    return dfi,dfv\n",
    "        \n",
    "        \n",
    "train_feature_index,train_feature_value = process(dfTrain,feature_dic)   \n",
    "test_feature_index,test_feature_value = process(dfTest,feature_dic)   \n",
    "\n",
    "\n",
    "dfm_params = {\n",
    "    'use_fm':True,\n",
    "    'use_deep':True,\n",
    "    'embedding_size':8,\n",
    "    'dropout_fm':[1,1],\n",
    "    'deep_layers':[32,32],\n",
    "    'dropout_deep':[0.5,0.5,0.5],\n",
    "    'deep_layer_activation':tf.nn.relu,\n",
    "    'epoch':30,\n",
    "    'learning_rate':0.01,\n",
    "    'batch_size':64,\n",
    "    'optimizer':'adam',\n",
    "    'batch_norm':1,\n",
    "    'batch_decay':0.95,\n",
    "    'l2_reg':0.01,\n",
    "    'verbose':True,\n",
    "    'eval_metric':'gini_norm',\n",
    "    'random_seed':3\n",
    "}\n",
    "\n",
    "dfm_params['feature_size'] = total_feature\n",
    "dfm_params['field_size'] = train_feature_index.shape[1]\n",
    "\n",
    "# 构建图\n",
    "tf.reset_default_graph()  \n",
    "\n",
    "\n",
    "feat_index = tf.placeholder(tf.int32,shape=[None,None],name='index')\n",
    "feat_value = tf.placeholder(tf.float32,shape=[None,None],name='value')\n",
    "label = tf.placeholder(tf.float32,shape=[None,1],name='label')\n",
    "\n",
    "weights = {}\n",
    "weights['feature_embeddings'] = tf.Variable(tf.random_normal([dfm_params['feature_size'],dfm_params['embedding_size']],0.,0.01),name='feature_embeddings')\n",
    "weights['feature_bias'] = tf.Variable(tf.random_normal([dfm_params['feature_size'],1],0.,0.1),name='feature_bias')\n",
    "\n",
    "\n",
    "num_layers = len(dfm_params['deep_layers'])\n",
    "input_size = dfm_params['field_size']*dfm_params['embedding_size']  # 每一个field做embedding\n",
    "glorot = np.sqrt(2./(input_size+dfm_params['deep_layers'][0]))\n",
    "weights['layer_0'] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=(input_size,dfm_params['deep_layers'][0])),dtype=np.float32)\n",
    "'''weights中bias取一个维度即可，且输入层不需要bias'''\n",
    "weights['bias_0'] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=(dfm_params['deep_layers'][0])),dtype=np.float32)\n",
    "\n",
    "for i in range(1,num_layers):\n",
    "    glorot = np.sqrt(2./(dfm_params['deep_layers'][i-1]+dfm_params['deep_layers'][i]))\n",
    "    weights['layer_'+str(i)] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=(dfm_params['deep_layers'][i-1],dfm_params['deep_layers'][i])),dtype=np.float32)\n",
    "    weights['bias_'+str(i)] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=(dfm_params['deep_layers'][i])),dtype=np.float32)\n",
    "\n",
    "'''最后一层为FM层+DNN最后一层'''\n",
    "input_size = dfm_params['deep_layers'][-1]+dfm_params['field_size']+dfm_params['embedding_size']\n",
    "glorot = np.sqrt(2./(input_size+1))\n",
    "\n",
    "weights['concat_project'] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=(input_size,1)),dtype=np.float32)\n",
    "weights['concat_bias'] = tf.Variable(tf.constant(0.01),dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''embedding'''\n",
    "embeddings = tf.nn.embedding_lookup(weights['feature_embeddings'],feat_index) \n",
    "reshaped_feat_value = tf.reshape(feat_value,[-1,dfm_params['field_size'],1])\n",
    "embeddings = tf.multiply(embeddings,reshaped_feat_value)\n",
    "\n",
    "'''fm part'''\n",
    "'''一阶'''\n",
    "fm_first_order = tf.nn.embedding_lookup(weights['feature_embeddings'],feat_index)\n",
    "fm_first_order = tf.reduce_sum(tf.multiply(fm_first_order,reshaped_feat_value),2)\n",
    "\n",
    "summed_feature_emb = tf.reduce_sum(embeddings,1)\n",
    "summed_feature_emb_square = tf.square(summed_feature_emb)\n",
    "\n",
    "squared_feature_emb = tf.square(embeddings)\n",
    "squared_sum_feature_emb = tf.reduce_sum(squared_feature_emb,1)\n",
    "\n",
    "fm_second_order = 0.5 * tf.subtract(summed_feature_emb_square,squared_sum_feature_emb)\n",
    "\n",
    "'''deep part'''\n",
    "y_deep = tf.reshape(embeddings,shape=[-1,dfm_params['field_size']*dfm_params['embedding_size']])\n",
    "\n",
    "for i in range(num_layers):\n",
    "    y_deep = tf.add(tf.matmul(y_deep,weights['layer_'+str(i)]),weights['bias_'+str(i)])\n",
    "    y_deep = tf.nn.relu(y_deep)\n",
    "\n",
    "concat_input = tf.concat([fm_first_order,fm_second_order,y_deep],axis=1)\n",
    "out = tf.nn.sigmoid(tf.add(tf.matmul(concat_input,weights['concat_project']),weights['concat_bias']))\n",
    "\n",
    "\n",
    "loss = tf.losses.log_loss(label,out)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate = dfm_params['learning_rate'],beta1=0.9,beta2=0.999,epsilon=1e-8).minimize(loss)\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(dfm_params['epoch']):\n",
    "        '''loss与train_op顺序不可换'''\n",
    "        epoch_loss,_ = sess.run([loss,train_op],feed_dict={feat_index:train_feature_index,feat_value:train_feature_value,label:y_train.reshape(-1,1)})\n",
    "        print('epoch:%s,Loss:%s'%(i,epoch_loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCN\n",
    "$$x_{l+1} = x_0 x_l^T w_l + b_l + x_l = f(x_l,w_l,b_l)+x_l$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-b2d6ac6f27ed>:111: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "Epoch0 : Loss 0.97\n",
      "Epoch1 : Loss 0.69\n",
      "Epoch2 : Loss 0.53\n",
      "Epoch3 : Loss 0.45\n",
      "Epoch4 : Loss 0.41\n",
      "Epoch5 : Loss 0.38\n",
      "Epoch6 : Loss 0.37\n",
      "Epoch7 : Loss 0.33\n",
      "Epoch8 : Loss 0.29\n",
      "Epoch9 : Loss 0.32\n",
      "Epoch10 : Loss 0.13\n",
      "Epoch11 : Loss 0.12\n",
      "Epoch12 : Loss 0.11\n",
      "Epoch13 : Loss 0.10\n",
      "Epoch14 : Loss 0.09\n",
      "Epoch15 : Loss 0.08\n",
      "Epoch16 : Loss 0.07\n",
      "Epoch17 : Loss 0.06\n",
      "Epoch18 : Loss 0.06\n",
      "Epoch19 : Loss 0.05\n",
      "Epoch20 : Loss 0.05\n",
      "Epoch21 : Loss 0.04\n",
      "Epoch22 : Loss 0.04\n",
      "Epoch23 : Loss 0.04\n",
      "Epoch24 : Loss 0.03\n",
      "Epoch25 : Loss 0.03\n",
      "Epoch26 : Loss 0.03\n",
      "Epoch27 : Loss 0.03\n",
      "Epoch28 : Loss 0.02\n",
      "Epoch29 : Loss 0.02\n"
     ]
    }
   ],
   "source": [
    "#DCN处理\n",
    "dfAll = pd.concat([dfTrain,dfTest])\n",
    "\n",
    "feature_dic = {}\n",
    "total_feature = 0\n",
    "for col in dfAll:\n",
    "    if col in IGNORE_COLS or col in NUMERIC_COLS:\n",
    "        continue\n",
    "    else:\n",
    "        unique_val = dfAll[col].unique()\n",
    "        feature_dic[col] = dict(zip(unique_val,range(total_feature,total_feature+len(unique_val))))\n",
    "        total_feature += len(unique_val)\n",
    "        \n",
    "'''\n",
    "主要将离散特征和连续特征分开，连续特征不在转换成embedding进行输入\n",
    "'''     \n",
    "def process(df,feature_dic):\n",
    "    num_val = df[NUMERIC_COLS].values  #.tolist()\n",
    "    '''只保留类别特征'''\n",
    "    dfi = df.copy()\n",
    "    dfi.drop(NUMERIC_COLS,axis=1,inplace=True)\n",
    "    dfv = dfi.copy()\n",
    "    for col in dfi.columns:\n",
    "        if col in IGNORE_COLS:\n",
    "            dfi.drop(col,axis=1,inplace=True)\n",
    "            dfv.drop(col,axis=1,inplace=True)\n",
    "        else:\n",
    "            dfi[col] = dfi[col].map(feature_dic[col])\n",
    "            dfv[col] = 1\n",
    "    dfi = dfi.values\n",
    "    dfv = dfv.values\n",
    "    \n",
    "    return dfi,dfv,num_val\n",
    "    \n",
    "train_cate_feature_index,train_cate_feature_value,train_num_feature_val = process(dfTrain,feature_dic)   \n",
    "test_cate_feature_index,test_feature_cate_value,test_num_feature_val = process(dfTest,feature_dic)   \n",
    "      \n",
    "\n",
    "dcn_params = {\n",
    "    \"embedding_size\": 8,\n",
    "    \"deep_layers\": [32, 32],\n",
    "    \"dropout_deep\": [0.5, 0.5, 0.5],\n",
    "    \"deep_layers_activation\": tf.nn.relu,\n",
    "    'loss' : 'logloss',\n",
    "    \"epoch\": 30,\n",
    "    \"batch_size\": 1024,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"optimizer_type\": \"adam\",\n",
    "    \"batch_norm\": 1,\n",
    "    \"batch_norm_decay\": 0.995,\n",
    "    \"l2_reg\": 0.01,\n",
    "    \"verbose\": True,\n",
    "    \"random_seed\": 2019,\n",
    "    \"cross_layer_num\":3\n",
    "}\n",
    "\n",
    "dcn_params['cate_feature_size'] = total_feature\n",
    "dcn_params['field_size'] = train_cate_feature_index.shape[1]\n",
    "dcn_params['numeric_feature_size'] = train_num_feature_val.shape[1]\n",
    "total_size = dcn_params['field_size']*dcn_params['embedding_size']+dcn_params['numeric_feature_size']  # 每一个field做embedding\n",
    "\n",
    "# 构建图\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(dcn_params['random_seed'])\n",
    "\n",
    "feat_index = tf.placeholder(tf.int32,shape=[None,None],name='feat_index')\n",
    "feat_value = tf.placeholder(tf.float32,shape=[None,None],name='feat_value')\n",
    "numeric_value = tf.placeholder(tf.float32,shape=[None,None],name='numeric_val')\n",
    "label = tf.placeholder(tf.float32,shape=[None,1],name='label')\n",
    "\n",
    "\n",
    "weights = {}\n",
    "weights['feature_embedding'] = tf.Variable(tf.random_normal(shape=[dcn_params['cate_feature_size'],dcn_params['embedding_size']],mean=0.,stddev=0.01),name='feature_embeddings')\n",
    "weights['feature_bias'] = tf.Variable(tf.random_normal(shape=[dcn_params['cate_feature_size']],mean=0,stddev=1),name='feature_bias')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''deep'''\n",
    "input_size = total_size\n",
    "glorot = np.sqrt(2./(input_size+dcn_params['deep_layers'][0]))\n",
    "weights['deep_layers_0'] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=[input_size,dcn_params['deep_layers'][0]]),dtype=np.float32,name='layers_0')\n",
    "weights['deep_bias_0'] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=[dcn_params['deep_layers'][0]]),dtype=np.float32,name='bias_0')\n",
    "\n",
    "\n",
    "num_layers = len(dcn_params['deep_layers'])\n",
    "for i in range(1,num_layers):\n",
    "    glorot = np.sqrt(2./(dcn_params['deep_layers'][i-1]+dcn_params['deep_layers'][i]))\n",
    "    weights['deep_layers_'+str(i)] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=[dcn_params['deep_layers'][i-1],dcn_params['deep_layers'][i]]),dtype=np.float32,name='deep_layers_'+str(i))\n",
    "    weights['deep_bias_'+str(i)] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=[dcn_params['deep_layers'][i]]),dtype=np.float32,name='deep_bias_'+str(i))\n",
    "\n",
    "\n",
    "# glorot = np.sqrt(2./(input_size*2))    # 初始化\n",
    "for i in range(dcn_params['cross_layer_num']):\n",
    "    weights['cross_layers_'+str(i)] = tf.Variable(np.random.normal(loc=0,scale=0.01,size=[total_size,1]),dtype=np.float32,name='cross_layers_'+str(i))\n",
    "    weights['cross_bias_'+str(i)] = tf.Variable(np.random.normal(loc=0,scale=0.01,size=[total_size,1]),dtype=np.float32,name='cross_bias_'+str(i))\n",
    "\n",
    "input_size += dcn_params['deep_layers'][-1]     \n",
    "glorot = np.sqrt(2./(input_size+1))\n",
    "weights['concat_projection'] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=[input_size,1]),dtype=np.float32,name='concat_projection')\n",
    "weights['concat_bias'] = tf.Variable(tf.constant(0.01),dtype=np.float32)                                                        \n",
    "                                                                                             \n",
    "                                                                                             \n",
    "embeddings = tf.nn.embedding_lookup(weights['feature_embedding'],feat_index)\n",
    "reshaped_feat_value = tf.reshape(feat_value,[-1,dcn_params['field_size'],1])\n",
    "embeddings = tf.multiply(embeddings,reshaped_feat_value)\n",
    "\n",
    "x0 = tf.concat([numeric_value,tf.reshape(embeddings,[-1,dcn_params['field_size']*dcn_params['embedding_size']])],axis=1)\n",
    "\n",
    "'''deep part'''\n",
    "y_deep = tf.nn.dropout(x0,dcn_params['dropout_deep'][0])\n",
    "for i in range(num_layers):\n",
    "    y_deep = tf.add(tf.matmul(y_deep,weights['deep_layers_'+str(i)]),weights['deep_bias_'+str(i)])\n",
    "    y_deep = tf.nn.relu(y_deep)\n",
    "    y_deep = tf.nn.dropout(y_deep,dcn_params['dropout_deep'][i+1])\n",
    "    \n",
    "'''cross part'''\n",
    "_x0 = tf.reshape(x0,[-1,total_size,1])\n",
    "x_l = _x0\n",
    "for l in range(dcn_params['cross_layer_num']):\n",
    "#     x_l = tf.tensordot(tf.matmul(_x0, x_l, transpose_b=True),\n",
    "#                                     weights[\"cross_layers_%d\" % l],1) + weights[\"cross_bias_%d\" % l] + x_l\n",
    "    # 注意计算顺序，可以加速很多\n",
    "    x_l = tf.tensordot(tf.reshape(x_l,[-1,1,total_size]),weights[\"cross_layers_%d\" % l],1) * _x0 + weights[\"cross_bias_%d\" % l] + x_l\n",
    "\n",
    "\n",
    "cross_network_out = tf.reshape(x_l,[-1,total_size])\n",
    "\n",
    "\n",
    "\n",
    "concat_input = tf.concat([y_deep,cross_network_out],axis=1)\n",
    "out = tf.add(tf.matmul(concat_input,weights['concat_projection']),weights['concat_bias'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if dcn_params['loss'] == 'logloss':\n",
    "    out = tf.nn.sigmoid(out)\n",
    "    loss = tf.losses.log_loss(label,out)\n",
    "elif dcn_params['loss'] == 'mse':\n",
    "    loss = tf.nn.l2_loss(tf.subtract(label,out))\n",
    "\n",
    "if dcn_params['l2_reg']>0:\n",
    "    loss += tf.contrib.layers.l2_regularizer(dcn_params['l2_reg'])(weights['concat_projection'])\n",
    "    for i in range(num_layers):\n",
    "        loss += tf.contrib.layers.l2_regularizer(dcn_params['l2_reg'])(weights['deep_layers_%d'%i])\n",
    "    for i in range(dcn_params['cross_layer_num']):\n",
    "        loss += tf.contrib.layers.l2_regularizer(dcn_params['l2_reg'])(weights['cross_layers_%d'%i])\n",
    "if dcn_params['optimizer_type'] == 'adam':\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate = dcn_params['learning_rate'],beta1=0.9,beta2=0.99,epsilon=1e-8).minimize(loss)\n",
    "elif dcn_params['optimizer_type'] == 'adagrad':\n",
    "    train_op = tf.train.AdagradOptimizer(learning_rate = dcn_params['learning_rate'],initial_accumulator_value=1e-8).minimize(loss)\n",
    "elif dcn_params['optimizer_type'] == 'gd':\n",
    "    train_op = tf.train.GradientDescentOptimizer(learning_rate=dcn_params['learning_rate']).minimize(loss)\n",
    "elif dcn_params['optimizer_type'] == 'momentum':\n",
    "    train_op = tf.train.MomentumOptimizer(learning_rate = dcn_params['learning_rate'],momentum=0.95).minimize(loss)\n",
    "        \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "def shuffle_in_union_scary(a,b,c,d):\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(b)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(c)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(d)\n",
    "\n",
    "def getbatch(Xi,Xv,Xv2,y,batch_size,index):\n",
    "    start = batch_size * index\n",
    "    end = min(batch_size * (index + 1),len(y))\n",
    "    return Xi[start:end],Xv[start:end],Xv2[start:end],y[start:end]\n",
    "  \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for e in range(dcn_params['epoch']):\n",
    "        shuffle_in_union_scary(train_cate_feature_index,train_cate_feature_value,train_num_feature_val,y_train)\n",
    "        for i in range(len(y_train)//dcn_params['batch_size']):\n",
    "            cate_Xi_batch,cate_Xv_batch,numeric_Xv_batch,y_batch = getbatch(train_cate_feature_index,train_cate_feature_value,train_num_feature_val,y_train,dcn_params['batch_size'],e)\n",
    "            '''注意取名，不能与loss重复'''\n",
    "            cur_loss,_ = sess.run([loss,train_op],feed_dict={feat_index:cate_Xi_batch,feat_value:cate_Xv_batch,numeric_value:numeric_Xv_batch,label:y_batch.reshape(-1,1)})\n",
    "        print('Epoch%i : Loss %.2f'%(e,cur_loss))\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
